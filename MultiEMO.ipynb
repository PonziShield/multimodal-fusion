{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMofbJh/tXiEfmrdolGie5e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5db80bdc0ee245248cd05b4f08bcfeed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2527f584be5b4055923d2a384940e879","IPY_MODEL_91a0e9c448ef481ba9534342cbebc370","IPY_MODEL_64c3452a26c24c5984b2da07d1d0cccf"],"layout":"IPY_MODEL_08f07633fdf54ae3bf64b95211aa0b31"}},"2527f584be5b4055923d2a384940e879":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca68e56c80054a26b733eaf36a2f05f4","placeholder":"​","style":"IPY_MODEL_c9a5416763f847a38648502cc721f691","value":"config.json: 100%"}},"91a0e9c448ef481ba9534342cbebc370":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f906aa2833f4564b9245522825428cd","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4bc4890f3024d39885712c7a04ff4e1","value":481}},"64c3452a26c24c5984b2da07d1d0cccf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c197cb112cb46d49d2c99bcab792274","placeholder":"​","style":"IPY_MODEL_f63c772379714ed3a7489ff7730bebff","value":" 481/481 [00:00&lt;00:00, 13.9kB/s]"}},"08f07633fdf54ae3bf64b95211aa0b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca68e56c80054a26b733eaf36a2f05f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9a5416763f847a38648502cc721f691":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f906aa2833f4564b9245522825428cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4bc4890f3024d39885712c7a04ff4e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c197cb112cb46d49d2c99bcab792274":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f63c772379714ed3a7489ff7730bebff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e038388218644487b11e861773b0272b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b63fb6a24bc041bda2080efef2ad0f21","IPY_MODEL_e74b2c1a3945402183af816052b42ec1","IPY_MODEL_b39e08d14bb04f65bb4f6e3d00f9608e"],"layout":"IPY_MODEL_3a466145cb3844919813dcb9080452fd"}},"b63fb6a24bc041bda2080efef2ad0f21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6f0a298eab42a0b7a06e1e173d0ea7","placeholder":"​","style":"IPY_MODEL_d34b1297e4c64f69a9b400f161fa4c29","value":"model.safetensors: 100%"}},"e74b2c1a3945402183af816052b42ec1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c2bfca143a34a79b6e9a43c017e97e5","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_982cc932b79e43ba90d408504b85afe3","value":498818054}},"b39e08d14bb04f65bb4f6e3d00f9608e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_876a784fd94049f89ca2aad82ebe9955","placeholder":"​","style":"IPY_MODEL_9ae827023da442dc80600b0dcadc8bc2","value":" 499M/499M [00:04&lt;00:00, 113MB/s]"}},"3a466145cb3844919813dcb9080452fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef6f0a298eab42a0b7a06e1e173d0ea7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d34b1297e4c64f69a9b400f161fa4c29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c2bfca143a34a79b6e9a43c017e97e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"982cc932b79e43ba90d408504b85afe3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"876a784fd94049f89ca2aad82ebe9955":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ae827023da442dc80600b0dcadc8bc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78fb53fff5154b87b918c04e0fc8b96a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69a093fad4654004bc34915f29eadbdc","IPY_MODEL_643755914261476cb7e9154514d764bd","IPY_MODEL_593f5670fa14427d877284e14ac10735"],"layout":"IPY_MODEL_fffcf522191b499ab73fb8b62d7402b7"}},"69a093fad4654004bc34915f29eadbdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_652c264d5798471580798eaae429d528","placeholder":"​","style":"IPY_MODEL_91722c282c4d4a5795c3586cc9a1f857","value":"vocab.json: 100%"}},"643755914261476cb7e9154514d764bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98add53f24e74483bfcfc3a521a5fa97","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31e680b3e21f4b4ca9bc27de2c1df240","value":898823}},"593f5670fa14427d877284e14ac10735":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50d2d65b47b14b80af667c5d97e53390","placeholder":"​","style":"IPY_MODEL_54f42f7215394b66bf0cde0956c8c0df","value":" 899k/899k [00:00&lt;00:00, 7.74MB/s]"}},"fffcf522191b499ab73fb8b62d7402b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"652c264d5798471580798eaae429d528":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91722c282c4d4a5795c3586cc9a1f857":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98add53f24e74483bfcfc3a521a5fa97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31e680b3e21f4b4ca9bc27de2c1df240":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50d2d65b47b14b80af667c5d97e53390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54f42f7215394b66bf0cde0956c8c0df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6db1605538e443a4b611480c5d299634":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_61cf3b47e8f148228df6da566fd4cfe0","IPY_MODEL_4985814e81e448e29dfd3fe9ade3f1d8","IPY_MODEL_c26f62150e1346409242db8a59226898"],"layout":"IPY_MODEL_1253f21558ef42ce8d02adf5af6deed7"}},"61cf3b47e8f148228df6da566fd4cfe0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_011ab2b9ef344cedbb5d7fec036368a5","placeholder":"​","style":"IPY_MODEL_5b08f51cf9064cc6baf811dff92585fe","value":"merges.txt: 100%"}},"4985814e81e448e29dfd3fe9ade3f1d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1078fce96773471495487d87095ff2ee","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_948a9a90b5b841cba0039c85c24a6338","value":456318}},"c26f62150e1346409242db8a59226898":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d220b0a3a9a847668a19f2a525da9976","placeholder":"​","style":"IPY_MODEL_b040ac103fd7492f86f9820498696a26","value":" 456k/456k [00:00&lt;00:00, 10.7MB/s]"}},"1253f21558ef42ce8d02adf5af6deed7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"011ab2b9ef344cedbb5d7fec036368a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b08f51cf9064cc6baf811dff92585fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1078fce96773471495487d87095ff2ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948a9a90b5b841cba0039c85c24a6338":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d220b0a3a9a847668a19f2a525da9976":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b040ac103fd7492f86f9820498696a26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b6a46ab7a9f46418bbf386379a7f3a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_303b29cf493e4d8da46c4b2c3eefdbec","IPY_MODEL_4d308e52e5624a7dbed237b830209da9","IPY_MODEL_6c58ecc4e56a48a6a325bc24b1adf94f"],"layout":"IPY_MODEL_f3ce589cd5c7488a9d41011fc3c68766"}},"303b29cf493e4d8da46c4b2c3eefdbec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38cc5f74c1c44279ad6d9efaea2a572c","placeholder":"​","style":"IPY_MODEL_11931dc6875d43d39da6a2c2adc27e0f","value":"tokenizer.json: 100%"}},"4d308e52e5624a7dbed237b830209da9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_01c66098cd1e46f7b7f706d7130a28d5","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d4899c7b4be64c5f823f2492d148808d","value":1355863}},"6c58ecc4e56a48a6a325bc24b1adf94f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d506321c2b242d88ac8e317e6dcfa06","placeholder":"​","style":"IPY_MODEL_cd2916fe369e4d5cb413badb54ca3067","value":" 1.36M/1.36M [00:00&lt;00:00, 25.9MB/s]"}},"f3ce589cd5c7488a9d41011fc3c68766":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38cc5f74c1c44279ad6d9efaea2a572c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11931dc6875d43d39da6a2c2adc27e0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"01c66098cd1e46f7b7f706d7130a28d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4899c7b4be64c5f823f2492d148808d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d506321c2b242d88ac8e317e6dcfa06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd2916fe369e4d5cb413badb54ca3067":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Zu-fQ0q7H235","executionInfo":{"status":"ok","timestamp":1703919828548,"user_tz":-330,"elapsed":5109,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn.functional import normalize\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import RobertaModel, RobertaTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from imblearn.under_sampling import RandomUnderSampler\n","from collections import Counter\n","from torch.optim.lr_scheduler import ExponentialLR\n","from sklearn.model_selection import train_test_split\n","\n","import re\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import WordPunctTokenizer"],"metadata":{"id":"SXRYrpgyPSNu","executionInfo":{"status":"ok","timestamp":1703919836862,"user_tz":-330,"elapsed":8315,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","import numpy as np\n","from sklearn.metrics import classification_report, f1_score, accuracy_score"],"metadata":{"id":"nxnvMDBUUIjv","executionInfo":{"status":"ok","timestamp":1703919836862,"user_tz":-330,"elapsed":13,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Check for GPU availability\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2eqBYOfrPU_b","executionInfo":{"status":"ok","timestamp":1703919836862,"user_tz":-330,"elapsed":11,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"4e965b8f-71ff-4d65-fb0a-1b456a499d10"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","source":["'''\n","Sample-Weighted Focal Contrastive (SWFC) Loss:\n","1. Divide training samples into positive and negative pairs to maximize\n","inter-class distances while minimizing intra-class distances;\n","2. Assign more importance to hard-to-classify positive pairs;\n","3. Assign more importance to minority classes.\n","'''\n","class SampleWeightedFocalContrastiveLoss(nn.Module):\n","\n","    def __init__(self, temp_param, focus_param, sample_weight_param, dataset, class_counts, device):\n","        '''\n","        temp_param: control the strength of penalty on hard negative samples;\n","        focus_param: forces the model to concentrate on hard-to-classify samples;\n","        sample_weight_param: control the strength of penalty on minority classes;\n","        dataset: MELD or IEMOCAP.\n","        device: cpu or cuda.\n","        '''\n","        super().__init__()\n","\n","        self.temp_param = temp_param\n","        self.focus_param = focus_param\n","        self.sample_weight_param = sample_weight_param\n","        self.dataset = dataset\n","        self.class_counts = class_counts\n","        self.device = device\n","\n","        if self.dataset == 'MELD':\n","            self.num_classes = 7\n","        elif self.dataset == 'IEMOCAP':\n","            self.num_classes = 6\n","        else:\n","            raise ValueError('Please choose either MELD or IEMOCAP')\n","\n","        self.class_weights = self.get_sample_weights()\n","\n","\n","    '''\n","    Use dot-product to measure the similarity between feature pairs.\n","    '''\n","    def dot_product_similarity(self, current_features, feature_sets):\n","        similarity = torch.sum(current_features * feature_sets, dim = -1)\n","        similarity_probs = torch.softmax(similarity / self.temp_param, dim = 0)\n","\n","        return similarity_probs\n","\n","\n","    '''\n","    Calculate the loss contributed from positive pairs.\n","    '''\n","    def positive_pairs_loss(self, similarity_probs):\n","        pos_pairs_loss = torch.mean(torch.log(similarity_probs) * ((1 - similarity_probs)**self.focus_param), dim = 0)\n","\n","        return pos_pairs_loss\n","\n","\n","    '''\n","    Assign more importance to minority classes.\n","    '''\n","    def get_sample_weights(self):\n","        total_counts = torch.sum(self.class_counts, dim = -1)\n","        class_weights = (total_counts / self.class_counts)**self.sample_weight_param\n","        class_weights = normalize(class_weights, dim = -1, p = 1.0)\n","\n","        return class_weights\n","\n","\n","    def forward(self, features, labels):\n","        self.num_samples = labels.shape[0]\n","        self.feature_dim = features.shape[-1]\n","\n","        features = normalize(features, dim = -1)  # normalization helps smooth the learning process\n","\n","        batch_sample_weights = torch.FloatTensor([self.class_weights[label] for label in labels]).to(self.device)\n","\n","        total_loss = 0.0\n","        for i in range(self.num_samples):\n","            current_feature = features[i]\n","            current_label = labels[i]\n","            feature_sets = torch.cat((features[:i], features[i + 1:]), dim = 0)\n","            label_sets = torch.cat((labels[:i], labels[i + 1:]), dim = 0)\n","            expand_current_features = current_feature.expand(self.num_samples - 1, self.feature_dim).to(self.device)\n","            similarity_probs = self.dot_product_similarity(expand_current_features, feature_sets)\n","            pos_similarity_probs = similarity_probs[label_sets == current_label]  # positive pairs with the same label\n","            if len(pos_similarity_probs) > 0:\n","                pos_pairs_loss = self.positive_pairs_loss(pos_similarity_probs)\n","                weighted_pos_pairs_loss = pos_pairs_loss * batch_sample_weights[i]\n","                total_loss += weighted_pos_pairs_loss\n","\n","        loss = - total_loss / self.num_samples\n","\n","        return loss"],"metadata":{"id":"G-XC3FjxIBUb","executionInfo":{"status":"ok","timestamp":1703919836862,"user_tz":-330,"elapsed":8,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["'''\n","Maximize the correlations across multimodal-fused features\n","extracted from MultiAttn through Soft-HGR loss.\n","'''\n","class SoftHGRLoss(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","\n","    '''\n","    Calculate the inner products between feature mappings.\n","    '''\n","    def feature_mapping(self, feature_X, feature_Y):\n","        feature_mapping_X_Y = torch.mean(torch.sum(feature_X * feature_Y, dim = -1), dim = 0)\n","\n","        return feature_mapping_X_Y\n","\n","\n","    '''\n","    Calculate the inner products between feature covariances.\n","    '''\n","    def feature_covariance(self, feature_X, feature_Y):\n","        cov_feature_X = torch.cov(feature_X)\n","        cov_feature_Y = torch.cov(feature_Y)\n","        # We empirically find that scaling the feature covariance by a factor of 1 / num_samples\n","        # leads to enhanced training stability and improvements in model performances.\n","        feature_covariance_X_Y = torch.trace(torch.matmul(cov_feature_X, cov_feature_Y)) / self.num_samples\n","        return feature_covariance_X_Y\n","\n","\n","    def forward(self, f_t, f_a, f_v):\n","        self.num_samples = f_t.shape[0]\n","\n","        all_features = [f_t, f_a, f_v]\n","        total_loss = 0.0\n","        for i in range(len(all_features) - 1):\n","            for j in range(i + 1, len(all_features)):\n","                feature_mapping_i_j = self.feature_mapping(all_features[i], all_features[j])\n","                feature_covariance_i_j = self.feature_covariance(all_features[i], all_features[j])\n","                soft_hgr_loss_i_j = feature_mapping_i_j - feature_covariance_i_j / 2\n","                total_loss += soft_hgr_loss_i_j\n","\n","        loss = - total_loss / self.num_samples\n","\n","        return loss"],"metadata":{"id":"FdOMGBVBIFCP","executionInfo":{"status":"ok","timestamp":1703919836862,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["'''\n","2-layer MLP with ReLU activation.\n","'''\n","class MLP(nn.Module):\n","\n","    def __init__(self, input_dim, hidden_dim, num_classes, dropout_rate):\n","        super().__init__()\n","\n","        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.linear_2 = nn.Linear(hidden_dim, num_classes)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","    def forward(self, x):\n","        return self.dropout(self.linear_2(self.relu(self.linear_1(x))))"],"metadata":{"id":"VdMYS2LYIIW0","executionInfo":{"status":"ok","timestamp":1703919836862,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["'''\n","Bidirectional cross-attention layers.\n","'''\n","class BidirectionalCrossAttention(nn.Module):\n","\n","    def __init__(self, model_dim, Q_dim, K_dim, V_dim):\n","        super().__init__()\n","\n","        self.query_matrix = nn.Linear(model_dim, Q_dim)\n","        self.key_matrix = nn.Linear(model_dim, K_dim)\n","        self.value_matrix = nn.Linear(model_dim, V_dim)\n","\n","\n","    def bidirectional_scaled_dot_product_attention(self, Q, K, V):\n","        score = torch.bmm(Q, K.transpose(-1, -2))\n","        scaled_score = score / (K.shape[-1]**0.5)\n","        attention = torch.bmm(F.softmax(scaled_score, dim = -1), V)\n","\n","        return attention\n","\n","\n","    def forward(self, query, key, value):\n","        Q = self.query_matrix(query)\n","        K = self.key_matrix(key)\n","        V = self.value_matrix(value)\n","        attention = self.bidirectional_scaled_dot_product_attention(Q, K, V)\n","\n","        return attention\n","\n","\n","\n","'''\n","Multi-head bidirectional cross-attention layers.\n","'''\n","class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, num_heads, model_dim, Q_dim, K_dim, V_dim):\n","        super().__init__()\n","\n","        self.num_heads = num_heads\n","        self.attention_heads = nn.ModuleList(\n","            [BidirectionalCrossAttention(model_dim, Q_dim, K_dim, V_dim) for _ in range(self.num_heads)]\n","        )\n","        self.projection_matrix = nn.Linear(num_heads * V_dim, model_dim)\n","\n","\n","    def forward(self, query, key, value):\n","        heads = [self.attention_heads[i](query, key, value) for i in range(self.num_heads)]\n","        multihead_attention = self.projection_matrix(torch.cat(heads, dim = -1))\n","\n","        return multihead_attention\n","\n","\n","\n","'''\n","A feed-forward network, which operates as a key-value memory.\n","'''\n","class Feedforward(nn.Module):\n","\n","    def __init__(self, model_dim, hidden_dim, dropout_rate):\n","        super().__init__()\n","\n","        self.linear_W1 = nn.Linear(model_dim, hidden_dim)\n","        self.linear_W2 = nn.Linear(hidden_dim, model_dim)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","    def forward(self, x):\n","        return self.dropout(self.linear_W2(self.relu(self.linear_W1(x))))\n","\n","\n","\n","'''\n","Residual connection to smooth the learning process.\n","'''\n","class AddNorm(nn.Module):\n","\n","    def __init__(self, model_dim, dropout_rate):\n","        super().__init__()\n","\n","        self.layer_norm = nn.LayerNorm(model_dim)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","\n","    def forward(self, x, sublayer):\n","        output = self.layer_norm(x + self.dropout(sublayer(x)))\n","\n","        return output\n","\n","\n","\n","'''\n","MultiAttn is a multimodal fusion model which aims to capture the complicated interactions and\n","dependencies across textual, audio and visual modalities through bidirectional cross-attention layers.\n","MultiAttn is made up of three sub-components:\n","1. MultiAttn_text: integrate the textual modality with audio and visual information;\n","2. MultiAttn_audio: incorporate the audio modality with textual and visual information;\n","3. MultiAttn_visual: fuse the visual modality with textual and visual cues.\n","'''\n","class MultiAttnLayer(nn.Module):\n","\n","    def __init__(self, num_heads, model_dim, hidden_dim, dropout_rate):\n","        super().__init__()\n","\n","        Q_dim = K_dim = V_dim = model_dim // num_heads\n","        self.attn_1 = MultiHeadAttention(num_heads, model_dim, Q_dim, K_dim, V_dim)\n","        self.add_norm_1 = AddNorm(model_dim, dropout_rate)\n","        self.attn_2 = MultiHeadAttention(num_heads, model_dim, Q_dim, K_dim, V_dim)\n","        self.add_norm_2 = AddNorm(model_dim, dropout_rate)\n","        self.ff = Feedforward(model_dim, hidden_dim, dropout_rate)\n","        self.add_norm_3 = AddNorm(model_dim, dropout_rate)\n","\n","\n","    def forward(self, query_modality, modality_A, modality_B):\n","        attn_output_1 = self.add_norm_1(query_modality, lambda query_modality: self.attn_1(query_modality, modality_A, modality_A))\n","        attn_output_2 = self.add_norm_2(attn_output_1, lambda attn_output_1: self.attn_2(attn_output_1, modality_B, modality_B))\n","        ff_output = self.add_norm_3(attn_output_2, self.ff)\n","\n","        return ff_output\n","\n","\n","\n","'''\n","Stacks of MultiAttn layers.\n","'''\n","class MultiAttn(nn.Module):\n","\n","    def __init__(self, num_layers, model_dim, num_heads, hidden_dim, dropout_rate):\n","        super().__init__()\n","\n","        self.multiattn_layers = nn.ModuleList([\n","            MultiAttnLayer(num_heads, model_dim, hidden_dim, dropout_rate) for _ in range(num_layers)])\n","\n","\n","    def forward(self, query_modality, modality_A, modality_B):\n","        for multiattn_layer in self.multiattn_layers:\n","            query_modality = multiattn_layer(query_modality, modality_A, modality_B)\n","\n","        return query_modality\n","\n","\n","\n","class MultiAttnModel(nn.Module):\n","\n","    def __init__(self, num_layers, model_dim, num_heads, hidden_dim, dropout_rate):\n","        super().__init__()\n","\n","        self.multiattn_text = MultiAttn(num_layers, model_dim, num_heads, hidden_dim, dropout_rate)\n","        self.multiattn_audio = MultiAttn(num_layers, model_dim, num_heads, hidden_dim, dropout_rate)\n","        self.multiattn_visual = MultiAttn(num_layers, model_dim, num_heads, hidden_dim, dropout_rate)\n","\n","\n","    def forward(self, text_features, audio_features, visual_features):\n","        f_t = self.multiattn_text(text_features, audio_features, visual_features)\n","        f_a = self.multiattn_audio(audio_features, text_features, visual_features)\n","        f_v = self.multiattn_visual(visual_features, text_features, audio_features)\n","\n","        return f_t, f_a, f_v"],"metadata":{"id":"RakiP-XHIh7u","executionInfo":{"status":"ok","timestamp":1703919836863,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class FakeBERT(nn.Module):\n","    def __init__(\n","        self,\n","        device,\n","        roberta_model_path='roberta-base',\n","        num_classes=1,\n","        inductor=True\n","        ):\n","        super(FakeBERT, self).__init__()\n","\n","        # Load pre-trained RoBERTa model\n","        self.roberta = RobertaModel.from_pretrained(roberta_model_path).to(device=device)\n","        if (inductor):\n","          self.roberta = torch.compile(self.roberta, backend=\"inductor\")\n","        self.tokenizer = RobertaTokenizer.from_pretrained(roberta_model_path)\n","\n","        # CNN\n","        self.conv1d_p1 = nn.Conv1d(in_channels=768, out_channels=128, kernel_size=5).to(device=device)\n","        self.conv1d_p2 = nn.Conv1d(in_channels=768, out_channels=128, kernel_size=4).to(device=device)\n","        self.conv1d_p3 = nn.Conv1d(in_channels=768, out_channels=128, kernel_size=3).to(device=device)\n","        self.conv1d_s1 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5).to(device=device)\n","        self.conv1d_s2 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5).to(device=device)\n","\n","        # Pooling\n","        self.max_pool_p1 = nn.MaxPool1d(kernel_size=5).to(device=device)\n","        self.max_pool_p2 = nn.MaxPool1d(kernel_size=5).to(device=device)\n","        self.max_pool_p3 = nn.MaxPool1d(kernel_size=5).to(device=device)\n","        self.max_pool_s1 = nn.MaxPool1d(kernel_size=5).to(device=device)\n","        self.max_pool_s2 = nn.MaxPool1d(kernel_size=10).to(device=device)\n","\n","        # Fully connected layers\n","        self.linear1 = nn.Linear(640, 128).to(device=device)\n","        self.linear2 = nn.Linear(128, num_classes).to(device=device)\n","        self.sigmoid = nn.Sigmoid().to(device=device)\n","\n","    def forward(self, x):\n","        # Tokenize and encode the sentences\n","        tokenized_sentences = self.tokenizer(x, truncation=True, padding='max_length', return_tensors='pt').to(device=device)\n","        # print('tokenized_sentences', tokenized_sentences.shape)\n","\n","        # Forward pass to get embeddings\n","        with torch.no_grad():\n","            # Get RoBERTa embeddings\n","            model_output = self.roberta(**tokenized_sentences)\n","\n","        # Extract embeddings from the output\n","        embeddings = model_output.last_hidden_state\n","        # print('embeddings', embeddings.shape)\n","\n","        output_p1 = self.max_pool_p1(F.relu(self.conv1d_p1(embeddings.permute(0, 2, 1))))\n","        output_p2 = self.max_pool_p2(F.relu(self.conv1d_p2(embeddings.permute(0, 2, 1))))\n","        output_p3 = self.max_pool_p3(F.relu(self.conv1d_p3(embeddings.permute(0, 2, 1))))\n","        output_s = torch.cat((output_p1, output_p2, output_p3), dim=2)\n","        output_s1 = F.relu(self.conv1d_s1(output_s))\n","        output_s1 = self.max_pool_s1(output_s1)\n","        output_s2 = F.relu(self.conv1d_s2(output_s1))\n","        output_s2 = self.max_pool_s2(output_s2)\n","        output_s2 = output_s2.permute(0, 2, 1)\n","        output_f = output_s2.reshape(output_s2.size(0), -1)\n","        output_l1 = torch.relu(self.linear1(output_f))\n","        output_l2 = self.linear2(output_l1)\n","        output = self.sigmoid(output_l2)\n","\n","        return output, output_l1"],"metadata":{"id":"G-Q4kyCgPNm_","executionInfo":{"status":"ok","timestamp":1703919836863,"user_tz":-330,"elapsed":7,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Initialize the model\n","fakebert = FakeBERT(\n","    device,\n","    inductor=False\n",")\n","\n","sentences = [\"I love this product!\"]\n","\n","outputs, before_sigmoid = fakebert(sentences)\n","\n","print(outputs.shape)\n","print(before_sigmoid.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264,"referenced_widgets":["5db80bdc0ee245248cd05b4f08bcfeed","2527f584be5b4055923d2a384940e879","91a0e9c448ef481ba9534342cbebc370","64c3452a26c24c5984b2da07d1d0cccf","08f07633fdf54ae3bf64b95211aa0b31","ca68e56c80054a26b733eaf36a2f05f4","c9a5416763f847a38648502cc721f691","8f906aa2833f4564b9245522825428cd","b4bc4890f3024d39885712c7a04ff4e1","8c197cb112cb46d49d2c99bcab792274","f63c772379714ed3a7489ff7730bebff","e038388218644487b11e861773b0272b","b63fb6a24bc041bda2080efef2ad0f21","e74b2c1a3945402183af816052b42ec1","b39e08d14bb04f65bb4f6e3d00f9608e","3a466145cb3844919813dcb9080452fd","ef6f0a298eab42a0b7a06e1e173d0ea7","d34b1297e4c64f69a9b400f161fa4c29","7c2bfca143a34a79b6e9a43c017e97e5","982cc932b79e43ba90d408504b85afe3","876a784fd94049f89ca2aad82ebe9955","9ae827023da442dc80600b0dcadc8bc2","78fb53fff5154b87b918c04e0fc8b96a","69a093fad4654004bc34915f29eadbdc","643755914261476cb7e9154514d764bd","593f5670fa14427d877284e14ac10735","fffcf522191b499ab73fb8b62d7402b7","652c264d5798471580798eaae429d528","91722c282c4d4a5795c3586cc9a1f857","98add53f24e74483bfcfc3a521a5fa97","31e680b3e21f4b4ca9bc27de2c1df240","50d2d65b47b14b80af667c5d97e53390","54f42f7215394b66bf0cde0956c8c0df","6db1605538e443a4b611480c5d299634","61cf3b47e8f148228df6da566fd4cfe0","4985814e81e448e29dfd3fe9ade3f1d8","c26f62150e1346409242db8a59226898","1253f21558ef42ce8d02adf5af6deed7","011ab2b9ef344cedbb5d7fec036368a5","5b08f51cf9064cc6baf811dff92585fe","1078fce96773471495487d87095ff2ee","948a9a90b5b841cba0039c85c24a6338","d220b0a3a9a847668a19f2a525da9976","b040ac103fd7492f86f9820498696a26","1b6a46ab7a9f46418bbf386379a7f3a3","303b29cf493e4d8da46c4b2c3eefdbec","4d308e52e5624a7dbed237b830209da9","6c58ecc4e56a48a6a325bc24b1adf94f","f3ce589cd5c7488a9d41011fc3c68766","38cc5f74c1c44279ad6d9efaea2a572c","11931dc6875d43d39da6a2c2adc27e0f","01c66098cd1e46f7b7f706d7130a28d5","d4899c7b4be64c5f823f2492d148808d","3d506321c2b242d88ac8e317e6dcfa06","cd2916fe369e4d5cb413badb54ca3067"]},"id":"ZxBwaSRRPdad","executionInfo":{"status":"ok","timestamp":1703919850215,"user_tz":-330,"elapsed":13359,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"abf0be78-fce4-43b8-f51f-30110e500331"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db80bdc0ee245248cd05b4f08bcfeed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e038388218644487b11e861773b0272b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78fb53fff5154b87b918c04e0fc8b96a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6db1605538e443a4b611480c5d299634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6a46ab7a9f46418bbf386379a7f3a3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1])\n","torch.Size([1, 128])\n"]}]},{"cell_type":"code","source":["torch.save(fakebert.state_dict(), '/content/fakebert-sentiment.pth')\n","torch.save(fakebert.state_dict(), '/content/fakebert-contract.pth')\n","torch.save(fakebert.state_dict(), '/content/fakebert-transaction.pth')"],"metadata":{"id":"BIiRF3oMPfO0","executionInfo":{"status":"ok","timestamp":1703919856628,"user_tz":-330,"elapsed":6421,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["'''\n","MultiEMO consists of three key components: unimodal context modeling, multimodal fusion, and emotion classification.\n","'''\n","class MultiEMO(nn.Module):\n","\n","    def __init__(self, device, multi_attn_flag=True, hidden_dim=1024, dropout=0, num_layers=6,\n","                 model_dim=128, num_heads=4, n_classes=1):\n","        super().__init__()\n","\n","        self.multi_attn_flag = multi_attn_flag\n","\n","        # TODO\n","        self.social_sentiment = FakeBERT(device=device, inductor=False)\n","        self.social_sentiment.load_state_dict(torch.load('/content/fakebert-sentiment.pth'))\n","        self.smart_contract = FakeBERT(device=device, inductor=False)\n","        self.smart_contract.load_state_dict(torch.load('/content/fakebert-contract.pth'))\n","        self.transaction = FakeBERT(device=device, inductor=False)\n","        self.transaction.load_state_dict(torch.load('/content/fakebert-transaction.pth'))\n","\n","\n","\n","        self.multiattn = MultiAttnModel(num_layers, model_dim, num_heads, hidden_dim, dropout)\n","\n","        self.fc = nn.Linear(model_dim * 3, model_dim)\n","\n","        # if self.dataset == 'MELD':\n","        #     self.mlp = MLP(model_dim, model_dim * 2, n_classes, dropout)\n","        # elif self.dataset == 'IEMOCAP':\n","        #     self.mlp = MLP(model_dim, model_dim, n_classes, dropout)\n","        self.mlp = MLP(model_dim, model_dim, n_classes, dropout)\n","\n","\n","    def forward(self, texts, contracts, transactions):\n","        _, sentiment_features = self.social_sentiment(texts)\n","        sentiment_features = sentiment_features.unsqueeze(1).repeat(1, 10, 1)\n","        print('sentiment_features', sentiment_features.shape)\n","        _, contract_features = self.smart_contract(contracts)\n","        contract_features = contract_features.unsqueeze(1).repeat(1, 10, 1)\n","        print('contract_features', contract_features.shape)\n","        _, transaction_features = self.transaction(transactions)\n","        transaction_features = transaction_features.unsqueeze(1).repeat(1, 10, 1)\n","        print('transaction_features', transaction_features.shape)\n","\n","        sentiment_features = sentiment_features.transpose(0, 1)\n","        print('sentiment_features', sentiment_features.shape)\n","        contract_features = contract_features.transpose(0, 1)\n","        print('contract_features', contract_features.shape)\n","        transaction_features = transaction_features.transpose(0, 1)\n","        print('transaction_features', transaction_features.shape)\n","\n","        if self.multi_attn_flag == True:\n","            fused_sentiment_features, fused_contract_features, fused_transaction_features = self.multiattn(sentiment_features, contract_features, transaction_features)\n","        else:\n","            fused_sentiment_features, fused_contract_features, fused_transaction_features = sentiment_features, contract_features, transaction_features\n","\n","        print('fused_sentiment_features', fused_sentiment_features.shape)\n","        print('fused_contract_features', fused_contract_features.shape)\n","        print('fused_transaction_features', fused_transaction_features.shape)\n","        fused_sentiment_features = fused_sentiment_features.reshape(-1, fused_sentiment_features.shape[-1])\n","        # fused_text_features = fused_text_features[padded_labels != -1]\n","        fused_contract_features = fused_contract_features.reshape(-1, fused_contract_features.shape[-1])\n","        # fused_audio_features = fused_audio_features[padded_labels != -1]\n","        fused_transaction_features = fused_transaction_features.reshape(-1, fused_transaction_features.shape[-1])\n","        # fused_visual_features = fused_visual_features[padded_labels != -1]\n","\n","        fused_features = torch.cat((fused_sentiment_features, fused_contract_features, fused_transaction_features), dim = -1)\n","        fc_outputs = self.fc(fused_features)\n","        mlp_outputs = self.mlp(fc_outputs)\n","\n","        return fused_sentiment_features, fused_contract_features, fused_transaction_features, fc_outputs, mlp_outputs"],"metadata":{"id":"4JFnyexNJIup","executionInfo":{"status":"ok","timestamp":1703920084179,"user_tz":-330,"elapsed":355,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Initialize the model\n","multiemo = MultiEMO(\n","    device=device\n",")\n","\n","sentences = [\"I love this product!\"]\n","\n","fused_text_features, fused_audio_features, fused_visual_features, fc_outputs, mlp_outputs = multiemo(sentences, sentences, sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WMy6HIUNCXH","executionInfo":{"status":"ok","timestamp":1703785002868,"user_tz":-330,"elapsed":14912,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"a4e91467-c261-4e6c-edd7-66ef0972f0e1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["sentiment_features torch.Size([1, 1, 128])\n","contract_features torch.Size([1, 1, 128])\n","transaction_features torch.Size([1, 1, 128])\n","fused_sentiment_features torch.Size([1, 1, 128])\n","fused_contract_features torch.Size([1, 1, 128])\n","fused_transaction_features torch.Size([1, 1, 128])\n"]}]},{"cell_type":"code","source":["print(\"fused_text_features shape:\", fused_text_features.shape)\n","print(\"fused_audio_features shape:\", fused_audio_features.shape)\n","print(\"fused_visual_features shape:\", fused_visual_features.shape)\n","print(\"fc_outputs shape:\", fc_outputs.shape)\n","print(\"mlp_outputs shape:\", mlp_outputs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwa6v2Wi2mDo","executionInfo":{"status":"ok","timestamp":1703785008220,"user_tz":-330,"elapsed":390,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"284dcd64-7c54-47e4-bc2a-afe6df62ee6d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["fused_text_features shape: torch.Size([1, 128])\n","fused_audio_features shape: torch.Size([1, 128])\n","fused_visual_features shape: torch.Size([1, 128])\n","fc_outputs shape: torch.Size([1, 128])\n","mlp_outputs shape: torch.Size([1, 1])\n"]}]},{"cell_type":"code","source":["# Initialize the model\n","multiemo = MultiEMO(\n","    device=device\n",")\n","\n","sentences = [\"I love this product!\"]\n","\n","fused_text_features, fused_audio_features, fused_visual_features, fc_outputs, mlp_outputs = multiemo(sentences, sentences, sentences)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CyOcuGIziwU9","executionInfo":{"status":"ok","timestamp":1703920099029,"user_tz":-330,"elapsed":11178,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"415f94ad-95b7-4377-b87c-fb997bc6d40f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["sentiment_features torch.Size([1, 10, 128])\n","contract_features torch.Size([1, 10, 128])\n","transaction_features torch.Size([1, 10, 128])\n","sentiment_features torch.Size([10, 1, 128])\n","contract_features torch.Size([10, 1, 128])\n","transaction_features torch.Size([10, 1, 128])\n","fused_sentiment_features torch.Size([10, 1, 128])\n","fused_contract_features torch.Size([10, 1, 128])\n","fused_transaction_features torch.Size([10, 1, 128])\n"]}]},{"cell_type":"code","source":["print(\"fused_text_features shape:\", fused_text_features.shape)\n","print(\"fused_audio_features shape:\", fused_audio_features.shape)\n","print(\"fused_visual_features shape:\", fused_visual_features.shape)\n","print(\"fc_outputs shape:\", fc_outputs.shape)\n","print(\"mlp_outputs shape:\", mlp_outputs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GmNRZu6ei2qW","executionInfo":{"status":"ok","timestamp":1703920104516,"user_tz":-330,"elapsed":481,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"350e0149-3b7a-4851-cddd-103c3a2a4fb6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["fused_text_features shape: torch.Size([10, 128])\n","fused_audio_features shape: torch.Size([10, 128])\n","fused_visual_features shape: torch.Size([10, 128])\n","fc_outputs shape: torch.Size([10, 128])\n","mlp_outputs shape: torch.Size([10, 1])\n"]}]},{"cell_type":"code","source":["sentences = [\"I love this product!\", \"It's terrible.\", \"Awesome experience.\", \"Worst ever.\", \"Great job!\", \"Awful.\", \"Excellent service.\", \"Hate it.\", \"Fantastic!\", \"Disappointing.\"]\n","labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n","\n","x_train, x_valid, y_train, y_valid = train_test_split(sentences, labels, test_size=0.2, stratify=labels, random_state=2023)\n","\n","# Create DataLoader for training and validation sets\n","batch_size = 128\n","\n","train_dataset = list(zip(x_train, y_train))\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","val_dataset = list(zip(x_valid, y_valid))\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","num_classes = 2\n","\n","def get_class_counts():\n","    class_counts = torch.zeros(num_classes).to(device)\n","\n","    for inputs, labels in train_loader:\n","        class_counts += torch.bincount(labels, minlength = num_classes)\n","\n","    return class_counts\n","\n","\n","# Initialize the model\n","SWFC_loss_param = 0.4\n","HGR_loss_param = 0.4\n","CE_loss_param = 0.2\n","sample_weight_param = 1.1\n","focus_param = 2.4\n","temp_param = 0.8\n","learning_rate = 0.0001\n","weight_decay = 0.00001\n","dataset = 'MELD'\n","class_counts = get_class_counts()\n","\n","\n","multiemo = MultiEMO(\n","    device=device\n",")\n","\n","SWFCLoss = SampleWeightedFocalContrastiveLoss(\n","    temp_param,\n","    focus_param,\n","    sample_weight_param,\n","    dataset,\n","    class_counts,\n","    device\n","    )\n","HGR_loss = SoftHGRLoss()\n","# CE_loss = nn.CrossEntropyLoss()\n","CELoss = nn.BCELoss().to(device=device)  # Binary Cross-Entropy Loss for binary classification\n","\n","optimizer = optim.Adam(multiemo.parameters(), lr = learning_rate, weight_decay = weight_decay)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.95, patience = 10, threshold = 1e-6, verbose = True)\n","sigmoid = nn.Sigmoid().to(device=device)\n","\n","# Training loop\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    multiemo.train()\n","    total_loss = 0.0\n","    total_SWFC_loss, total_HGR_loss, total_CE_loss = 0.0, 0.0, 0.0\n","    all_labels, all_preds = [], []\n","\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        fused_text_features, fused_audio_features, fused_visual_features, fc_outputs, mlp_outputs = multiemo(inputs, inputs, inputs)\n","\n","        soft_HGR_loss = HGR_loss(fused_text_features, fused_audio_features, fused_visual_features).to(device=device)\n","        # print('soft_HGR_loss', soft_HGR_loss)\n","        # print('labels', labels)\n","        # print('fc_outputs', fc_outputs)\n","        SWFC_loss = SWFCLoss(fc_outputs, labels).to(device=device)\n","        # print('SWFC_loss', SWFC_loss)\n","        # print('mlp_outputs', mlp_outputs)\n","        output = sigmoid(mlp_outputs)\n","        # print('output', output)\n","        CE_loss = CELoss(output, torch.as_tensor(labels, dtype=torch.float32).unsqueeze(1).to(device=device))\n","        # print('CE_loss', CE_loss)\n","\n","        loss = soft_HGR_loss * HGR_loss_param + SWFC_loss * SWFC_loss_param + CE_loss * CE_loss_param\n","\n","        total_loss += loss.item()\n","\n","        total_HGR_loss += soft_HGR_loss.item()\n","        total_SWFC_loss += SWFC_loss.item()\n","        total_CE_loss += CE_loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # preds = torch.argmax(mlp_outputs, dim = -1)\n","        preds = (output > 0.5).int()\n","        # print('preds', preds)\n","        # print('labels', labels)\n","        all_labels.append(labels.cpu().numpy())\n","        all_preds.append(preds.view(-1).cpu().numpy())\n","\n","    all_labels = np.concatenate(all_labels)\n","    all_preds = np.concatenate(all_preds)\n","    avg_f1 = round(f1_score(all_labels, all_preds, average = 'weighted', zero_division=1) * 100, 4)\n","    print('avg_f1', avg_f1)\n","    avg_acc = round(accuracy_score(all_labels, all_preds) * 100, 4)\n","    print('avg_acc', avg_acc)\n","    report = classification_report(all_labels, all_preds, digits = 4, zero_division=1)\n","    print('report', report)"],"metadata":{"id":"gsGZTkHA4GY-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703788445099,"user_tz":-330,"elapsed":257863,"user":{"displayName":"Nimsara Fernando","userId":"11470797706788239513"}},"outputId":"b1be0ad9-6512-4253-933f-6e52c693fb73"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["sentiment_features torch.Size([8, 1, 128])\n","contract_features torch.Size([8, 1, 128])\n","transaction_features torch.Size([8, 1, 128])\n","fused_sentiment_features torch.Size([8, 1, 128])\n","fused_contract_features torch.Size([8, 1, 128])\n","fused_transaction_features torch.Size([8, 1, 128])\n","avg_f1 33.3333\n","avg_acc 50.0\n","report               precision    recall  f1-score   support\n","\n","           0     0.5000    1.0000    0.6667         4\n","           1     1.0000    0.0000    0.0000         4\n","\n","    accuracy                         0.5000         8\n","   macro avg     0.7500    0.5000    0.3333         8\n","weighted avg     0.7500    0.5000    0.3333         8\n","\n","sentiment_features torch.Size([8, 1, 128])\n","contract_features torch.Size([8, 1, 128])\n","transaction_features torch.Size([8, 1, 128])\n","fused_sentiment_features torch.Size([8, 1, 128])\n","fused_contract_features torch.Size([8, 1, 128])\n","fused_transaction_features torch.Size([8, 1, 128])\n","avg_f1 33.3333\n","avg_acc 50.0\n","report               precision    recall  f1-score   support\n","\n","           0     1.0000    0.0000    0.0000         4\n","           1     0.5000    1.0000    0.6667         4\n","\n","    accuracy                         0.5000         8\n","   macro avg     0.7500    0.5000    0.3333         8\n","weighted avg     0.7500    0.5000    0.3333         8\n","\n","sentiment_features torch.Size([8, 1, 128])\n","contract_features torch.Size([8, 1, 128])\n","transaction_features torch.Size([8, 1, 128])\n","fused_sentiment_features torch.Size([8, 1, 128])\n","fused_contract_features torch.Size([8, 1, 128])\n","fused_transaction_features torch.Size([8, 1, 128])\n","avg_f1 33.3333\n","avg_acc 50.0\n","report               precision    recall  f1-score   support\n","\n","           0     1.0000    0.0000    0.0000         4\n","           1     0.5000    1.0000    0.6667         4\n","\n","    accuracy                         0.5000         8\n","   macro avg     0.7500    0.5000    0.3333         8\n","weighted avg     0.7500    0.5000    0.3333         8\n","\n","sentiment_features torch.Size([8, 1, 128])\n","contract_features torch.Size([8, 1, 128])\n","transaction_features torch.Size([8, 1, 128])\n","fused_sentiment_features torch.Size([8, 1, 128])\n","fused_contract_features torch.Size([8, 1, 128])\n","fused_transaction_features torch.Size([8, 1, 128])\n","avg_f1 33.3333\n","avg_acc 50.0\n","report               precision    recall  f1-score   support\n","\n","           0     1.0000    0.0000    0.0000         4\n","           1     0.5000    1.0000    0.6667         4\n","\n","    accuracy                         0.5000         8\n","   macro avg     0.7500    0.5000    0.3333         8\n","weighted avg     0.7500    0.5000    0.3333         8\n","\n","sentiment_features torch.Size([8, 1, 128])\n","contract_features torch.Size([8, 1, 128])\n","transaction_features torch.Size([8, 1, 128])\n","fused_sentiment_features torch.Size([8, 1, 128])\n","fused_contract_features torch.Size([8, 1, 128])\n","fused_transaction_features torch.Size([8, 1, 128])\n","avg_f1 87.3016\n","avg_acc 87.5\n","report               precision    recall  f1-score   support\n","\n","           0     1.0000    0.7500    0.8571         4\n","           1     0.8000    1.0000    0.8889         4\n","\n","    accuracy                         0.8750         8\n","   macro avg     0.9000    0.8750    0.8730         8\n","weighted avg     0.9000    0.8750    0.8730         8\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PyITxxEDfMgS"},"execution_count":null,"outputs":[]}]}